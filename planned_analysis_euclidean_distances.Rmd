---
title: "R Notebook"
output: html_notebook
---


```{r setup}
library(readxl) #for reading excel
library(corrplot) #for correlation plot
library(glmnet) #for lasso and ridge regression (for variable selection)
```

Read in and clean data according to eda findings.

```{r read_in_data}
t1_minus_t2 <- read_excel("t1_minus_t2.xlsx")



#This extracts the euclidean distances along with the patient identifiers.

numcolmax <- length(t1_minus_t2[1, ]) #original number of columns
distances <- as.data.frame(t1_minus_t2[ , c(1, seq(5, numcolmax, by = 4))]) #extracting, turning into dataframe

#str(distances) #look at; I commented this out using "#"; you can remove this



#Now I clean the data

#columns were not in order (left and right were not necessarily beside each other); fixed using sort

distances <- distances[ , c(1, order(names(distances[, 2:45])) + 1)] #sorting, not including the first column

#str(distances) #look at; manually sorting it is; I commented this out using "#"; you can remove this

distances_sorted <- distances[ , c(1:20, 22, 21, 23, 24, 27, 25, 28, 26, 29, 32, 35, 30, 33, 31, 34, 36, 40, 37, 41, 38, 42, 39, 43, 44, 45)] #here I am sorting manually; this could be rearranged in excel as well before starting

#str(distances_sorted) #look at; it looks good now

# distances_sorted$k_r_dist[22] <- NA #replacing extreme outlier with NA
# 
# distances_sorted$k_l_dist[22] <- NA #paired observation
# 
# distances_sorted$u6_l_furc_dist[19] <- NA #replacing extreme outlier with NA
# 
# distances_sorted$u6_r_furc_dist[19] <- NA #paired observation



#Dataframe for asymmetries, named after the left landmark

asym <- distances_sorted[ , 1]
for (landmark in 1:22){
   asym <- as.data.frame(cbind(asym, as.vector(abs(distances_sorted[, (2*landmark)] - distances_sorted[, (2*landmark + 1)]))))
}

names(asym) <- names(distances_sorted[ , c(1, seq(2, 44, by = 2))])

```

Below I start on analysis I; first I need to do some manipulating of data so that it is in the correct form

```{r analysis_I_plot}

for (landmark_plot in 1:22){
  plot(distances_sorted[ , 2*landmark_plot], distances_sorted[ , 2*landmark_plot + 1], #plots points
       ylim = c(0,6), xlim = c(0,6), #axis
       ylab = "Right Landmark", xlab = "Left Landmark", main = names(asym)[landmark_plot + 1], #labels
       pch = 16, cex = .9) #point shape and size
  
  abline(a = 0, b = 1, lty = 2, col = "green") #dotted lines (lty = 2) with slope 'b' and y-intercept 'a'
  abline(a = 1, b = 1, lty = 2, col = "orange")
  abline(a = 2, b = 1, lty = 2, col = "red")
  abline(a = -1, b = 1, lty = 2, col = "orange")
  abline(a = -2, b = 1, lty = 2, col = "red")
}

```

Asymmetry would be seen as observations far from the green dotted line.

green: no asymmetry; orange: 1mm of asymmetry; red: 2mm of asymmetry.

There looks like there are some asymmetries here in some landmarks; however, this asymmetry isn't greater than 2mm on average for any of the landmarks. This suggests that we should decrease our threshold for testing. Perhaps try 1mm? Some landmarks may show significant asymmetry in that case.


Now I will formally test if the average asymmetry of any landmark is greater than 1mm (this can be adjusted to other values).

I begin by giving p-values of this test for each landmark. As we are doing many tests, I will use a bonferonni correction. There is significant asymmetry if any p-value is less than 0.05/22 = 0.00227 (if testing with 95% confidence).

```{r analysis_I_test}
for(landmark in 1:22){
  print(t.test(asym[ , landmark + 1], alternative = "greater",
         mu = 1)$p.value) #This '1' is the value that you may want to change depending on what you would like to test
}

```

The test does not suggest that there are any significant asymmetries. However, I would suggest retry-ing the analysis with more data. More data may reveal that there is significant asymmetry (at the 1mm level) with the 'u6_mbcusp' landmark or other landmarks, or it may reveal that the lower p-values seen here are just because of chance.

We next look at the asymmetries across all landmarks as a vector for each individual.

```{r}

asym

```

Finally, for each landmark separately, we look at the absolute asymmetry for each individual and model this as a linear function of different demographics and surgical measures. 


```{r read_in_regression_data}
demographics <- as.data.frame(read_excel("demographics.xlsx"))

demographics[ , 4] <- as.factor(demographics[ , 4])
demographics[ , 5] <- as.factor(demographics[ , 5])
demographics[ , 7] <- as.factor(demographics[ , 7])
demographics[ , 8] <- as.factor(demographics[ , 8])
demographics[ , 9] <- as.factor(demographics[ , 9])
demographics[ , 10] <- as.factor(demographics[ , 10])

demographics <- cbind(demographics, asym)

```

Just quickly looking below to see if we should transform any explanatory variables. I see no need for any transformations here.

```{r regression_model_assumptions}

for (demographic in 2:10) {
  plot(demographics[, 1], demographics[ , demographic])
}

```

```{r correlations}

removed_NA_patients <- demographics[ , colSums(is.na(demographics)) == 0] #removed landmarks with any NA's for the graphic

M = cor(removed_NA_patients[,-c(1,11, 4, 5, 7, 8, 9, 10)]) #removing patient identifiers and categorical variables for graphic
corrplot(M, method = 'color', order = 'FPC', type = 'lower', diag = FALSE) #there is a very high correlation between the two ages which needs to be accounted for

sum(demographics[, 7]==demographics[, 8]) #ie. it agrees in 25 out of 26 patients

sum(demographics[, 9]==demographics[, 10], na.rm = TRUE) #ie. it agrees in 20 out of 26 patients

M[1,2] #a very strong correlation between age
```
I now run a regression model to predict the absolute asymmetry within each landmark. I handle highly correlated variables by adding interaction terms (for age) or removing one of the variables (for the two categorical variables).

```{r regression_models}
pvalues_coeff <- vector(mode = "numeric", length = 10)
coeff <- vector(mode = "numeric", length = 10)

for (landmark in 12:33) {

  small_dataset <- demographics[ , c(2:10)]
  first_model <- lm(demographics[, landmark] ~ . + `T1 age`:`T2 age`-`PMD right`-`right posterior TAD # cortices`, data = small_dataset)

  print(summary(first_model))
  
  pvalues_coeff <- cbind(pvalues_coeff, summary(first_model)$coefficients[,4])
  
  coeff <- cbind(coeff, summary(first_model)$coefficients[,1])
  
}

rowSums(pvalues_coeff)
rowSums(coeff)/22

```

Just to try it, I next try a backwards selection technique based on the sum of the p-values. This is not common practice, and is more of an exploratory measure.

I first remove age.
```{r regression_models_removed_age}
pvalues_coeff <- vector(mode = "numeric", length = 7)

coeff <- vector(mode = "numeric", length = 7)

for (landmark in 12:33) {

  small_dataset <- demographics[ , c(2:10)]
  first_model <- lm(demographics[, landmark] ~ . -`T1 age`-`T2 age`-`PMD right`-`right posterior TAD # cortices`, data = small_dataset)

  print(summary(first_model))
  
  pvalues_coeff <- cbind(pvalues_coeff, summary(first_model)$coefficients[,4])
  
  coeff <- cbind(coeff, summary(first_model)$coefficients[,1])
  
  
}

rowSums(pvalues_coeff)

rowSums(coeff)/22

```

Some linear models are significant (barely). However, once you adjust for multiple testing, this significance fades. Just to get a rough idea if perhaps one explanatory variable may be worth looking into further, I summed the p-values across all models. The rough motivation is that if one of these explanatory variables are predictive of asymmetry in one landmark, it should also be predictive in many landmarks. Thus, perhaps if one variable shows more promise than others across all of the models, it can be looked into further. However, I saw no evidence of this.



```{r regression_models_removed_age_sig_model}

names(demographics)[18]

small_dataset <- demographics[ , c(2:10)]
first_model <- lm(demographics[, 18] ~ . -`T1 age`-`T2 age`-`PMD right`-`right posterior TAD # cortices`, data = small_dataset)

print(summary(first_model))

plot(first_model)


```
There is an interesting result that can be seen here for the landmark l6_cg. The linear model explains the asymmetry well (according to a model p-value of 0.0008). Significant effects were found from extractions, with an extraction increasing asymmetry on average by 0.5 mm and number of turns decreasing asymmetry by 0.03 mm per turn. However, across the linear models for all landmarks, the average modeled effect of an extraction increased asymmetry on average by 0.28 mm and number of turns increased asymmetry by 0.01 mm per turn. Based on this, I believe the interaction between extractions and asymmetry is worth further investigation.

